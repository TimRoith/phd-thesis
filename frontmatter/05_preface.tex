\chapter{Preface}\label{ch:preface}

This work is structured into two main parts, \cref{part:Intro} the presentation and explanation of the topics and results presented in \cref{part:Prints}, the peer-reviewed articles.

%
%
\begin{center}%
\begin{tikzpicture}
\hypersetup{linkcolor=black}%
\filldraw[fill=sky!20, draw=none] (0,11) rectangle (7.,10); %
\node[draw=none, align=center] at (3.5,10.5) {\cref{part:Intro}: \nameref{part:Intro}};
%
\filldraw[fill=sky!10, draw=none] (0,10) rectangle (7,9); %
\node[draw=none, align=center] at (3.5,9.5) {%
\parbox{.45\textwidth}{\small\cref{ch:para}: \nameref{ch:para}}};
%
\draw[draw=sky!30, thick] (0,9) -- (7, 9); %
%
\filldraw[fill=sky!10, draw=none] (0,9) rectangle (7,8); %
\node[draw=none, align=center] at (3.5,8.5) {%
\parbox{.45\textwidth}{\small\cref{ch:SSL}: \nameref{ch:SSL}}};
%
\draw[draw=sky!30, thick] (0,8) -- (7, 8); %
%
\filldraw[fill=sky!10, draw=none] (0,8) rectangle (7,7); %
\node[draw=none, align=center] at (3.5,7.5) {%
\parbox{.45\textwidth}{\small\cref{ch:SL}: \nameref{ch:SL}}};
%
%
%
\filldraw[fill=apple!20, draw=none] (7.5,11) rectangle (14.5,10);%
\node[draw=none, align=center] at (11.,10.5) {\cref{part:Prints}: \nameref{part:Prints}};
%
\filldraw[fill=apple!10, draw=none] (7.5,10) rectangle (14.5,9); %
\node[draw=none, align=center] at (11.,9.5) {---% 
};
%
\draw[draw=apple!30, thick] (7.5,9) -- (14.5, 9); %
%
\filldraw[fill=apple!10, draw=none] (7.5,9) rectangle (14.5,8); %
\node[draw=none, align=center] at (11.,8.5) {%
{\small\cref{paper:roith2022continuum,paper:bungert2021uniform}}};
%
\draw[draw=apple!30, thick] (7.5,8) -- (14.5, 8); %
%
\filldraw[fill=apple!10, draw=none] (7.5,8) rectangle (14.5,7); %
\node[draw=none, align=center] at (11.,7.5) {%
{\small\cref{paper:bungert2022bregman,paper:bungert2021clip,paper:kabri2023resolution}}};
\end{tikzpicture}
\end{center}
%
%
\cref{part:Intro} consists of five chapters, of which the first two give an introduction and explain the paradigms of \emph{unsupervised}, \emph{semi-supervised} and \emph{supervised} learning. The next two chapters are split up thematically, concerning the topics of semi-supervised and supervised learning, respectively. Here, a short overview provides the necessary framework, allowing us to explain the main contributions. The last chapter presents the conclusion. In \cref{part:Prints} the following publications are reprinted:


\printbibliography[keyword={papersA}, resetnumbers=true, heading=none]
\printbibliography[keyword={papersB}, resetnumbers=true, heading=none]

\noindent%
The following two works that are not part of this thesis but provide an additional insight.

\printbibliography[keyword={papersC}, resetnumbers=true, heading=none]

\subsection*{TR's Contribution}

Here we list TR's contribution to the publications included in the thesis.

\paragraph{\cite{roith2022continuum}:} This work builds upon the findings in TR's master's thesis \cite{roith2022msc}. It is however important to note that the results constitute a significant extension and are conceptually stronger than the ones in \cite{roith2022msc}, see \cref{sec:GConv}. TR adapted the continuum limit framework to the $L^\infty$ case, worked out most of the proofs and wrote a significant part of the paper. In collaboration with LB, he identified the crucial domain assumptions that allow to work on non-convex domains and proved convergence for approximate boundary conditions.

\paragraph{\cite{bungert2021uniform}:} In collaboration with LB, TR worked on the convergence proofs building upon the ideas of JC. Together with LB and JC he proved the main convergence result and the various lemmas leading up to it. Here, he was especially concerned with the adaptation of the theory of AMLEs to the graph case, with is a crucial element for the whole work. Furthermore, he contributed to the design and implementation of the numerical examples conducted in the paper. 

\paragraph{\cite{bungert2021clip}:} TR worked out the main algorithm proposed in the paper together with LB, based on LB's idea. Together with LS, RR and DT he conducted the numerical examples and also wrote large parts of the source code. Furthermore, he wrote significant parts of the paper, where DT proofread and clarified the final document.

\paragraph{\cite{bungert2022bregman}:} TR expanded LB's ideas of employing Bregman iteration for sparse training, conceptualized by DT. Together with MB and LB, he worked out the convergence analysis of stochastic Bregman iterations. Here, he also proposed a profound sparse initialization strategy. Furthermore, he conducted the numerical examples and wrote most of the source code.

\paragraph{\cite{kabri2023resolution}:} This work is based on SK's master's thesis, employing the initial ideas of MB for resolution invariance with FNOs. In the paper, TR worked out the proofs for well-definedness and Fr√©chet-differentiability, together with SK. He wrote large parts of the paper and the source code, where DT helped with proofreading of the published version. Here, he conducted the numerical studies in collaboration with SK.

