\chapter{Conclusion}\label{ch:C}
%
%
This thesis provided insights into the topics of consistency, sparsity and robustness of learning algorithms. Thematically, the thesis is split into two main chapters, dealing with semi-supervised and supervised learning. We summarize both chapters below and respectively provide an outlook and possible future work.

\paragraph{Consistency of SSL on Sparse Graphs} We considered the infinite data limit in the semi-supervised setting, focusing on the Lipschitz learning task. For all our results we assumed mild scaling conditions which allow for very sparse graphs. We were first able to show $\Gamma$-convergence in the variational setting. We then proved convergence rates for AMLEs via a homogenization strategy, which relied on the comparison with cones principle. The key insight we obtained was, that a rate for graph distance functions implies a rate for graph AMLEs. This observation was already employed in our follow-up work in \cite{bungert2022ratio}, where convergence rates at an even smaller scale were shown. We also conducted experiments to validate our theoretical framework in practice. Here, we observed better results than we were able to prove. 

While our framework allows the graph to be very sparse, we are restricted to the case of ball graphs. Here, it would be interesting to see, how our results can be transferred to the knn setting as in \cite{calder2022improved}. Furthermore, we only considered the standard Lipschitz learning task, which tends to forget the data distribution. In \cite{calder2019consistency} a modification was proposed that makes the problem sensitive to the distribution. The open problem that arises here is how to adapt the technique in \cite{bungert2021uniform} to show convergence for the modified problem.\par

\paragraph{Robust and Sparse SL} In the supervised setting, we considered input-robustness w.r.t. adversarial perturbations and resolution changes. In the first case, we proposed a defense mechanism to train stable neural networks, based on Lipschitz regularization. We provided analytical results and numerical experiments that suggest that the strategy allows us to learn robust networks. The strength of this approach depends on how well the discrete Lipschitz constant approximates the true one. An interesting future direction would be to explore different methods to determine the discrete set of Lipschitz pairs. E.g. one could additionally learn a generative model that outputs these pair in a faster and probably even more expressive way than the gradient descent scheme, employed right now.

For the multi-resolution setting, we analyzed the role of Fourier neural operators. We first showed certain functional analytic properties of neural layers acting on $L^p$ spaces. We then established the relation of disctretized Fourier layers to standard convolution operations, both from a theoretical and a practical side. We also highlighted the importance of the trigonometric interpolation in this context. We conducted numerical tests that suggest the resolution equivariant behavior of FNO layers. Connected to the previous topic it would be interesting to study the adversarial robustness of FNOs. First numerical tests in \cite{kabri2022FNO} hint that the vulnerabilities are even worse than in the standard case.

We considered the question of how to enforce sparsity in neural network weights. Here, we employed a stochastic variant of Bregman iterations, which allowed us to train networks that are very sparse throughout the whole optimization. We provided theoretic convergence guarantees, where the main novelty was the stochasticity introduced into the iteration. We also showed the numerical efficiency of the method, by training sparse neural networks performing similarly to their dense counterparts. One open problem is to weaken the convexity assumption and instead prove the convergence result, by employing a Kurdykaâ€“\L{}ojasiewicz type inequality as in \cite{benning2018choose}. Concerning the neural architecture search via sparsity as proposed in our work, a further interesting task would be to learn the U-Net type architecture of \cite{ronneberger2015u}.
