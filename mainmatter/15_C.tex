\chapter{Conclusion}\label{ch:SL}
%
%
This thesis provided insights in the topics of consistency, sparsity and robustness of learning algorithms. We first considered the infinite data limit in the semi-supervised setting. Focusing on the Lipschitz learning task we were first able to show $\Gamma$-convergence in the variational setting. We then proved convergence rates for AMLEs and provided some numerical results. In the supervised setting, we considered input-robustness w.r.t. adversarial perturbations and resolution changes. In the first case we proposed a defense mechanism to train stable neural networks. In the multi-resolution setting we analyzed the role of Fourier neural operators both from the theoretical and practical side. Furthermore, we considered the question on how to enforce sparsity in the network weights. Here, we employed a stochastic variant of Bregman iterations for which we provided convergence guarantees. We also showed the numerical efficiency of the method.

All of the works above spark